{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Making a disease diagnosis from a sample of tissue (Pathology) is part art/part science. It relies on quantitative metrics such as counts of scores of miss-formed cell morphology, but also on a final subjective decision by the physician. This decision may or may not be formed from a objective weighting of the quantitative metrics. \n",
    "\n",
    "### My Investigation\n",
    "To gain insight into this decision, I anaylzed data from the wisconsin breast cancer database and compared the conclusions of the pathologist with the results of machine learning algorithims: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original). \n",
    "\n",
    "### Hypothesis\n",
    "By inspecting the feature space, and visually comparing this to the diagnosis labels, I found that the pathological assessment appears to be formed from a roughly linear combination of the recorded cytological features. To test this I built a machine learning classifier and compared its performance to the results of the pathologist. The accuracy, precision and recall of this the logistic model were all very high. To gain further insight I identify the misclassified data and suggest they be re-inspected by the pathologist to determine the traits that distinguish them.\n",
    "\n",
    "### Findings\n",
    "A simple logistic model trained on 80% of the data (and tested on the remaining 20%) achieves a cross-validation accuracy and test accuracy of 0.96, (very good). Other metrics Accuracy 97.143, Precision 94.737 and Recall 98.182 are also high and ROC AUC of 0.98 (also very good).\n",
    "\n",
    "The three most important features are 1.Clump Thickness, 2. Single Epithelial Cell Size, and 3. Bland Chromatin In fact, when running a classifier on these three columns alone we achieve a nearly identical confusion matrix, F1 score (0.954), and ROC AUC (0.946). Because of this, resources could be saved by measuring only these three features, and the model obtained would perform just as well.\n",
    "Once put into production, a clinician could input these three (or 9) numbers into my logistic regression model to diagnose.\n",
    "\n",
    "\n",
    "### Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"id\", \"clump_thickness\", \"cell_size\", \"cell_shape\", \"marginal_adhesion\", \n",
    "           \"epithelial_cell_size\", \"bare_nuclei\", \"bland_chromatin\", \"normal_nucleoli\", \"mitoses\", \"class\"]\n",
    "\n",
    "data = pd.read_csv(\"breast-cancer-wisconsin.data.csv\", names=columns)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {2: 0, 4: 1}\n",
    "data[\"class\"] = data[\"class\"].apply(lambda x: labels[x])\n",
    "#data.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram every feature\n",
    "\n",
    "It looks like for all the predictors the data span the full range from 0 to 10. and the distributions are similar (except for bland_chromatin, clump_thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.describe()\n",
    "hist = data.hist(bins=11, figsize=(13, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data.drop([\"id\"], axis = 1)\n",
    "tmp.boxplot()\n",
    "plt.xticks(rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I want to see if any of the data look like they are correlated, have other non-linear relationships or have strongly non-normal distributions. Since the number of features is relatively low, its easy to visualize this in a scatter-matrix plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize = (20,20))\n",
    "plts = pd.plotting.scatter_matrix(data,figsize = (10,10))\n",
    "\n",
    "for x in range(len(plts)):\n",
    "    for y in range(len(plts)):\n",
    "        # to get the axis of subplots\n",
    "        ax = plts[x, y]\n",
    "        # to make x axis name vertical  \n",
    "        ax.xaxis.label.set_rotation(90)\n",
    "        # to make y axis name horizontal \n",
    "        ax.yaxis.label.set_rotation(0)\n",
    "        # to make sure y axis names are outside the plot area\n",
    "        ax.yaxis.labelpad = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are a few pairwise correlations between a couple of features e.g. (cell Size and cell shape). Most of the variables have a preponderance of '1s' but are otherwise rather evenly spread across the remaining 2-10 range. Since most feature scaling won't do anything to redistribute data out of the 1's bucket it's better to leave the data as is. The exception perhaps being 'Mitosis' that has a bit of a tail but most models should be fairly robust to this small amount of skew, so I'll proceed without rescaling the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(data)\n",
    "data[\"bare_nuclei\"]= data[\"bare_nuclei\"].replace('?', np.nan)\n",
    "data[\"bare_nuclei\"]= data.fillna(data[\"bare_nuclei\"].median())\n",
    "data[\"bare_nuclei\"]= data.bare_nuclei.astype('int64')\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many of each class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data[\"class\"].value_counts()\n",
    "counts.plot(kind = \"bar\");\n",
    "plt.xlabel(\"class\");\n",
    "plt.ylabel(\"count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(data):\n",
    "    \"\"\"extract the features from the pandas dataframe \n",
    "    and convert it into a nxm matrix of m observations\n",
    "    of n features. I am wrapping this in a function\n",
    "    so to encapsulate my feature engineering here.\"\"\"\n",
    "    X =  np.vstack((data.clump_thickness,\n",
    "                     data.cell_shape,\n",
    "                     data.marginal_adhesion,\n",
    "                     data.epithelial_cell_size,\n",
    "                     data.bland_chromatin,\n",
    "                     data.normal_nucleoli,\n",
    "                     data.mitoses)).T\n",
    "    return np.sqrt(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate what sort of modeling might be need it's worth first looking at a low dimentional projection of the data using PCA to see how the class I am predicting is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = make_features(data)\n",
    "y = data[\"class\"]\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "X_low = pca.transform(X)\n",
    "\n",
    "plt.plot(X_low[:,0][y == 0],X_low[:,1][y == 0],'o')\n",
    "plt.plot(X_low[:,0][y == 1],X_low[:,1][y == 1],'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "It looks like a linear decision boundary should do a fairly good job. I will now approach this as a supervised machine learning problem to predict the class y from the data X, because the data look roughly linearly separable I'll start with a simple logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = data.drop([\"id\", \"class\"], axis = 1).as_matrix()\n",
    "y = data[\"class\"]\n",
    "\n",
    "# Illustrated here for manual splitting of training and testing data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Standarize features\n",
    "#scaler = StandardScaler()\n",
    "#X_train= scaler.fit_transform(X_train)\n",
    "#X_test= scaler.transform(X_test)\n",
    "\n",
    "# define method\n",
    "logreg= LogisticRegression(multi_class='multinomial', class_weight='balanced', solver='newton-cg')\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "score = logreg.score(X_train, y_train)\n",
    "print('model train accuracy:', score)\n",
    "\n",
    "# Use cross_val_score to automatically split, fit, and score.\n",
    "scores = cross_val_score(logreg, X_train, y_train, cv=10)\n",
    "#print(scores)\n",
    "print('model cross vaildation accuracy:{}'.format(scores.mean()))\n",
    "\n",
    "\n",
    "#plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "\n",
    "#classes =[-2, 0, 2]\n",
    "y_pred = logreg.predict(X_test)\n",
    "#cm_analysis(y_test, y_pred, classes, ymap=None, figsize=(8,8))\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "score = logreg.score(X_test, y_test)\n",
    "print('model test accuracy: {}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\", round(accuracy_score(y_test, y_pred) * 100, 3))\n",
    "print(\"Precision\", round(precision_score(y_test, y_pred) * 100, 3))\n",
    "print(\"Recall\", round(recall_score(y_test, y_pred) * 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrepretability\n",
    "\n",
    "Now, i am looking to help the decision maker explain the features can contribute to the prediction.  I am using the package LIME to make sample-by-sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-4b829066145b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'pathological'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'benign'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlime_tabular\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLimeTabularExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscretize_continuous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, training_data, mode, training_labels, feature_names, categorical_features, categorical_names, kernel_width, kernel, verbose, class_names, feature_selection, discretize_continuous, discretizer, sample_around_instance, random_state, training_data_stats)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 self.discretizer = QuartileDiscretizer(\n\u001b[1;32m    215\u001b[0m                         \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                         self.feature_names, labels=training_labels)\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mdiscretizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'decile'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 self.discretizer = DecileDiscretizer(\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/lime/discretize.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, categorical_features, feature_names, labels, random_state)\u001b[0m\n\u001b[1;32m    185\u001b[0m         BaseDiscretizer.__init__(self, data, categorical_features,\n\u001b[1;32m    186\u001b[0m                                  \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                                  random_state=random_state)\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/lime/discretize.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, categorical_features, feature_names, labels, random_state, data_stats)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# To override when implementing a custom binning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/lime/discretize.py\u001b[0m in \u001b[0;36mbins\u001b[0;34m(self, data, labels)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_discretize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mqts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mbins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2486\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2487\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "np.random.seed(1)\n",
    "\n",
    "column_names = [\"clump_thickness\", \"cell_size\", \"cell_shape\", \"marginal_adhesion\", \n",
    "           \"epithelial_cell_size\", \"bare_nuclei\", \"bland_chromatin\", \"normal_nucleoli\", \"mitoses\"]\n",
    "target_names = ['pathological', 'benign']\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(train, feature_names= column_names, class_names= target_names, discretize_continuous=True)\n",
    "\n",
    "for num in range(5):\n",
    "    i = np.random.randint(0, test.shape[0])\n",
    "    exp = explainer.explain_instance(test[i], rf.predict_proba, num_features=2, top_labels=1)\n",
    "    exp.show_in_notebook(show_table=True, show_all= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is a lot going on here. First, note that the row we are explained is displayed on the right side, in table format. Since we had the show_all parameter set to false, only the features used in the explanation are displayed.\n",
    "\n",
    "The value column displays the original value for each feature.\n",
    "\n",
    "Note that LIME has discretized the features in the explanation. This is because we let discretize_continuous=True in the constructor (this is the default). Discretized features make for more intuitive explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
